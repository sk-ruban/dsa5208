{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pyenq7obG4le"
   },
   "source": [
    "We will split the notebook into the following five sections:\n",
    "1. Data preparation\n",
    "2. Set up MPI processes\n",
    "3. Applying distributed kernel ridge regression\n",
    "4. Obtaining predicted median value using kernel function\n",
    "5. Evaluating model performance\n",
    "6. Cross Validation / Model Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkMqw3GRHz74"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip3 install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip3 install mpi4py\n",
    "\n",
    "# Import Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpi4py import MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ge6TJIcHaRp"
   },
   "source": [
    "## **Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "HhYbDDPvG8FK",
    "outputId": "19131633-5f98-4a81-902e-6bdba9f12f55"
   },
   "outputs": [],
   "source": [
    "#Import Data\n",
    "df = pd.read_csv('housing.tsv', sep='\\t', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "\n",
    "df.columns = ['longitude', 'latitude', 'housingMedianAge', 'totalRooms',\n",
    "              'totalBedrooms', 'population', 'households', 'medianIncome',\n",
    "              'oceanProximity', 'medianHouseValue']\n",
    "\n",
    "              \n",
    "# Display basic information\n",
    "print(df.info())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtocUfNLH-kh"
   },
   "outputs": [],
   "source": [
    "#Normalize Data\n",
    "\n",
    "features = ['longitude', 'latitude', 'housingMedianAge', 'totalRooms',\n",
    "              'totalBedrooms', 'population', 'households', 'medianIncome',\n",
    "              'oceanProximity', 'medianHouseValue']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "print(\"\\nDataset after preprocessing:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data\n",
    "\n",
    "X = df[features]\n",
    "y = df['medianHouseValue']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Set up MPI Processes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MPI communicator, rank, etc.\n",
    "\n",
    "comm = MPI.COMM_WORLD       #Initializes the communicator for all parallel processes in the MPI environment\n",
    "rank = comm.Get_rank()      #This command gets the rank of the current process\n",
    "size= comm.Get_size()       #This command gets the total number of processes that are running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data distribution into parallel processes, pt.1 - split the data\n",
    "\n",
    "if rank == 0:                                               #On the master process, process 0, let's handle the initial data distribution\n",
    "    X_chunks = np.array_split(X_train, size)                #Split data by number of processes (a.k.a. size)\n",
    "    y_chunks = np.array_split(y_train, size)\n",
    "else:\n",
    "    X_chunks = None                                         #All other processes do not have access to the full dataset at the start.\n",
    "    y_chunks = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data distribution into parallel processes, pt.2 - scatter the data\n",
    "\n",
    "X_local = comm.scatter(X_chunks, root=0)    #X_local is the portion of X_chunks allocated to the local process\n",
    "y_local = comm.scatter(y_chunks, root=0)    #Same for y_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5r6WMHeofS_"
   },
   "source": [
    "## **Applying Kernel Ridge Regression**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74ZsY2geomwH"
   },
   "outputs": [],
   "source": [
    "#Define Kernel Computation Function\n",
    "def compute_gaussian_kernel(X1, X2, sigma):\n",
    "    X1 = np.array(X1)\n",
    "    X2 = np.array(X2)\n",
    "    dists = np.sum(X1**2, axis=1).reshape(-1, 1) + np.sum(X2**2, axis=1) - 2 * np.dot(X1, X2.T) # |x1-x2|^2 = (x1)^2 + (x2)^2 -2(x1 . x2)\n",
    "    return np.exp(-dists / (2 * sigma ** 2))                                                    # exp( - dists / 2.sigma )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Circular communication for kernel computation. \n",
    "'''\n",
    "Meaning, iteratively, process 0 dot process 0, then process 0 dot process 1, then process 0 dot process 2, then append them together. \n",
    "That's process 0's total output row of kernel compute. \n",
    "We need to send the data from each process to another process in a circular way, so each process gets a new chunk of unseen data to dot with.\n",
    "'''\n",
    "\n",
    "kernel_matrix_local = np.zeros((X_local.shape[0], size)) #Each process will build a row of the full kernel matrix (of size x size)\n",
    "\n",
    "#Circular Communication for kernel computation\n",
    "\n",
    "for round in range(size):\n",
    "    if round == 0:                   #In round 0, each process will compare with itself\n",
    "        X_other_local = X_local\n",
    "    else: \n",
    "        X_other_local = comm.sendrecv(X_local, dest=(rank-1)%size, source=(rank+1)%size)   \n",
    "        '''\n",
    "        Remember that modulo is defined as: a % b = a - (a // b) * b\n",
    "        #But in python, the floor function for a negative a //b makes it more negative. This elicits a certain circular behaviour:\n",
    "        #Eg. in round 1, if you have 4 total processes, process 0 must send to process 3. \n",
    "        #Destination for process 0: (0-1)%4 = -1%4 = (-1) - (-1)(4) = 3!\n",
    "        '''\n",
    "\n",
    "    #Compute the kernel value (dot product of two local data chunks)\n",
    "    kernel_value = compute_gaussian_kernel(X_local, X_other_local, sigma=1.0)\n",
    "\n",
    "    #print(kernel_value)\n",
    "\n",
    "    #Determine where to put this local kernel_value in the final output row for each process\n",
    "    index = (rank + round) % size\n",
    "    kernel_matrix_local[:, index] = np.diagonal(kernel_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's do regularization and conjugate gradient computation in parallel. So we will put off mpi.gather till later.\n",
    "\n",
    "'''\n",
    "\n",
    "#Gather all the rows back to form the full kernel matrix K at root process 0\n",
    "\n",
    "if rank == 0:\n",
    "    full_kernel_matrix = np.zeroes((X_local.shape[0] * size, size))\n",
    "else:\n",
    "    full_kernel_matrix = None\n",
    "\n",
    "#MPI gather \n",
    "\n",
    "comm.Gather(kernel_matrix_local, full_kernel_matrix, root = 0)\n",
    "\n",
    "if rank == 0:\n",
    "    print(full_kernel_matrix)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain distributed matrix A\n",
    "\n",
    "# kernel_matrix_local is the distributed portion of K for each process\n",
    "# Add regularization term lambda * I locally\n",
    "\n",
    "lambda_value = 0.1  #Temporary lambda value\n",
    "\n",
    "# Assuming each process has a chunk of the matrix (rows)\n",
    "# global_start_index is the starting row index for each process in the global matrix\n",
    "global_start_index = rank * kernel_matrix_local.shape[0]  # e.g., rank 0 handles rows 0,1; rank 1 handles rows 2,3, etc.\n",
    "\n",
    "for i in range(kernel_matrix_local.shape[0]):\n",
    "    global_index = global_start_index + i  # Compute the global diagonal index\n",
    "    kernel_matrix_local[i, i] += lambda_value if global_index == global_start_index + i else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solve for alpha, using A(alpha) = y, and the Conjugate Gradient Method\n",
    "\n",
    "# Matrix-vector multiplication of A * p\n",
    "def mv_mul(A_local, p_global, comm):\n",
    "    #Each process computes its local contribution to the matrix-vector multiplication\n",
    "    p_local = p_global[rank * A_local.shape[0] : (rank+1) * A_local.shape[0]]    #P is the direction of movement for our solution\n",
    "    w_local = np.dot(A_local, p_local)\n",
    "\n",
    "    #Gather results from all processes to form the full result vector w_global\n",
    "    w_global = np.zeros_like(p_global)\n",
    "    comm.Allgather(w_local, w_global)\n",
    "\n",
    "    return w_global\n",
    "\n",
    "\n",
    "# Inner product for (r.T * r) or (p.T * w), using MPI_Allreduce for global sum\n",
    "def inner_product(v_local1, v_local2, comm):\n",
    "    local_dot_product = np.dot(v_local1, v_local2)\n",
    "    global_dot_product = comm.allreduce(local_dot_product, op=MPI.SUM)\n",
    "    return global_dot_product\n",
    "\n",
    "\n",
    "# Compute conjugate gradient algorithm process, returns alpha_local \n",
    "def conjugate_gradient_distributed(K_local, y_local, tol=1e-5, max_iter=1000):\n",
    "\n",
    "    #Initialize alpha (solution) locally as zero\n",
    "    alpha_local = np.zeros_like(y_local)\n",
    "\n",
    "    #Initialize residual, r = y-K @ alpha. Initially alpha is 0, so r = y\n",
    "    r_local = y_local.copy()\n",
    "\n",
    "    #Initialize direction p = r\n",
    "    p_local = r_local.copy()\n",
    "\n",
    "    #Compute initial squared norm of the residual r\n",
    "    SE = inner_product(r_local, r_local, comm)\n",
    "\n",
    "    #Start iterations\n",
    "    for iteration in range(max_iter):\n",
    "        if rank == 0:\n",
    "            print(f\"Iteration {iteration}, residual norm: {np.sqrt(SE)}\")\n",
    "        if np.sqrt(SE) < tol:\n",
    "            break\n",
    "\n",
    "        #Matrix vector product of w = K @ p\n",
    "        w_global = mv_mul(K_local, p_local, comm)\n",
    "\n",
    "        #Compute dot product p.T w\n",
    "        s = inner_product(p_local, w_local, comm)\n",
    "\n",
    "        #Update step size: alpha_Step = (r.T r)/(p.T K p)\n",
    "        alpha_step = SE / s\n",
    "\n",
    "        #Update alpha: alpha = alpha + alpha_step * p\n",
    "        alpha_local += alpha_step * p_local\n",
    "\n",
    "        #Update residual: r = r - alpha_step * w\n",
    "        r_local -= alpha_step * w_global\n",
    "\n",
    "        #Compute new squared error/squared norm of residual r\n",
    "        new_SE = inner_product(r_local, r_local, comm)\n",
    "\n",
    "        #Compute new beta\n",
    "        beta = new_SE / SE\n",
    "\n",
    "        #Update direction p: p = r + beta * p\n",
    "        p_local = r_local + beta * p_local\n",
    "\n",
    "        #Update SE variable\n",
    "        SE = new_SE\n",
    "\n",
    "    #Gather results from all processes to generate alpha\n",
    "    #alpha_global = np.hstack(comm.allgather(alpha_local))\n",
    "\n",
    "    return alpha_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_local = conjugate_gradient_distributed(kernel_matrix_local, y_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our estimator function f(x) is defined as SUM to N of (alpha_i . k(x,x_i) ). We use that to predict median value!\n",
    "\n",
    "def f_estimator(X, X_local, alpha_local, comm):\n",
    "\n",
    "    #Broadcast X to all processes\n",
    "    X = comm.bcast(X, root = 0)\n",
    "\n",
    "    #Number of training points N\n",
    "    N = alpha_global.shape[0]\n",
    "\n",
    "    #Initialize the local sum\n",
    "    local_sum = 0.0\n",
    "\n",
    "    #Compute local sum of alpha_i . K(x, x_i)\n",
    "    for i in range(X_local.shape[0]):\n",
    "        \n",
    "        local_sum += alpha_global[i] * compute_gaussian_kernel(X, X_local[i], sigma=1.0)  #Not sure, feel like this is wrong, we need to find for indiv x value\n",
    "\n",
    "    #Use MPI_reduce to sum up results in all processes\n",
    "    global_sum = comm.reduce(local_sum, op=MPI.SUM, root=0)\n",
    "\n",
    "    return global_sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Obtaining predicted median value using kernel function / assess model performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(f_estimator, y_test, x_test, alpha_global, comm):\n",
    "\n",
    "    N = len(y_test)\n",
    "    f_values = np.zeros(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        x_i = x_test[i]\n",
    "        f_values[i] = f_estimator(x_test, x_i, alpha_global, comm)\n",
    "\n",
    "    rmse = np.sqrt( np.mean(f_values - y_training) **2 )\n",
    "\n",
    "    return rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rank == 0:\n",
    "    rmse = compute_rmse(f_estimator, y_test, x_test, alpha_global, comm)\n",
    "    print(rmse)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
